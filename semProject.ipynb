{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from statistics import mode\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import LancasterStemmer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,Attention\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rs41AaG2Ewj",
        "outputId": "0b15529d-09c0-4479-8acf-c3a82f2112bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMnbMIES69rI",
        "outputId": "715397e7-164e-48d1-b56a-0d005a5eb62e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        " \n",
        "df = pd.read_csv(\"/content/drive/MyDrive/datasets/Reviews.csv\", nrows = 100000)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G33tlH2i0_Hg",
        "outputId": "cada0293-a67b-4c44-d143-a4fd9c70a821"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Id   ProductId          UserId                      ProfileName  \\\n",
            "0           1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
            "1           2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
            "2           3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
            "3           4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
            "4           5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
            "...       ...         ...             ...                              ...   \n",
            "99995   99996  B000LQORDE  A2P7HIRYYWVOBD                            Mason   \n",
            "99996   99997  B000LQORDE  A1K0ZH5MQFBA77                       jennilight   \n",
            "99997   99998  B000LQORDE  A29FRN2O7LWINL                          T. Tsai   \n",
            "99998   99999  B000LQORDE   A9Q950IPXJR1D          Lynda \"casual customer\"   \n",
            "99999  100000  B000LQORDE  A19W47CXJJP1MI               Amazonian Consumer   \n",
            "\n",
            "       HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
            "0                         1                       1      5  1303862400   \n",
            "1                         0                       0      1  1346976000   \n",
            "2                         1                       1      4  1219017600   \n",
            "3                         3                       3      2  1307923200   \n",
            "4                         0                       0      5  1350777600   \n",
            "...                     ...                     ...    ...         ...   \n",
            "99995                     2                       5      5  1254096000   \n",
            "99996                     2                       5      4  1250985600   \n",
            "99997                     2                       5      5  1237766400   \n",
            "99998                     2                       5      4  1237161600   \n",
            "99999                     2                       5      5  1235088000   \n",
            "\n",
            "                                                 Summary  \\\n",
            "0                                  Good Quality Dog Food   \n",
            "1                                      Not as Advertised   \n",
            "2                                  \"Delight\" says it all   \n",
            "3                                         Cough Medicine   \n",
            "4                                            Great taffy   \n",
            "...                                                  ...   \n",
            "99995                                             yummy!   \n",
            "99996                                  Tastes like More!   \n",
            "99997                                        Great ramen   \n",
            "99998                                            Spicy!!   \n",
            "99999  This spicy noodle cures my cold, upset stomach...   \n",
            "\n",
            "                                                    Text  \n",
            "0      I have bought several of the Vitality canned d...  \n",
            "1      Product arrived labeled as Jumbo Salted Peanut...  \n",
            "2      This is a confection that has been around a fe...  \n",
            "3      If you are looking for the secret ingredient i...  \n",
            "4      Great taffy at a great price.  There was a wid...  \n",
            "...                                                  ...  \n",
            "99995  I just love it and will buy another box when I...  \n",
            "99996  My late father in law used to have a rating sy...  \n",
            "99997  This is my favorite brand of Korean ramen. It ...  \n",
            "99998  I do like these noodles although, to say they ...  \n",
            "99999  I love this noodle and have it once or twice a...  \n",
            "\n",
            "[100000 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bYSJu8o9uUIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the duplicate and na values from the records\n",
        "df.drop_duplicates(subset=['Text'],inplace=True)\n",
        "df.dropna(axis=0,inplace=True)\n",
        "input_data = df.loc[:,'Text']\n",
        "target_data = df.loc[:,'Summary']\n",
        "target_data.replace('', np.nan, inplace=True)"
      ],
      "metadata": {
        "id": "Y1Nc4K771vaS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts=[]\n",
        "target_texts=[]\n",
        "input_words=[]\n",
        "target_words=[]\n",
        "contractions=pickle.load(open(\"contractions.pkl\",\"rb\"))['contractions']\n",
        "#initialize stop words and LancasterStemmer\n",
        "stop_words=set(stopwords.words('english'))\n",
        "stemm=LancasterStemmer()"
      ],
      "metadata": {
        "id": "NzSEiTVg2gxl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(texts,src):\n",
        "  #remove the html tags\n",
        "  texts = BeautifulSoup(texts, \"lxml\").text\n",
        "  #tokenize the text into words \n",
        "  words=word_tokenize(texts.lower())\n",
        "  #filter words which contains \\ \n",
        "  #integers or their length is less than or equal to 3\n",
        "  words= list(filter(lambda w:(w.isalpha() and len(w)>=3),words))\n",
        "  #contraction file to expand shortened words\n",
        "  words= [contractions[w] if w in contractions else w for w in words ]\n",
        "  #stem the words to their root word and filter stop words\n",
        "  if src==\"inputs\":\n",
        "    words= [stemm.stem(w) for w in words if w not in stop_words]\n",
        "  else:\n",
        "    words= [w for w in words if w not in stop_words]\n",
        "  return words"
      ],
      "metadata": {
        "id": "3Qcefx7c3YEW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pass the input records and taret records\n",
        "for in_txt,tr_txt in zip(input_data,target_data):\n",
        "  in_words= clean(in_txt,\"inputs\")\n",
        "  input_texts+= [' '.join(in_words)]\n",
        "  input_words+= in_words\n",
        "  #add 'sos' at start and 'eos' at end of text\n",
        "  tr_words= clean(\"sos \"+tr_txt+\" eos\",\"target\")\n",
        "  target_texts+= [' '.join(tr_words)]\n",
        "  target_words+= tr_words\n",
        "\n",
        "#store only unique words from input and target list of words\n",
        "input_words = sorted(list(set(input_words)))\n",
        "target_words = sorted(list(set(target_words)))\n",
        "num_in_words = len(input_words) #total number of input words\n",
        "num_tr_words = len(target_words) #total number of target words\n",
        "\n",
        "#get the length of the input and target texts which appears most often  \n",
        "max_in_len = mode([len(i) for i in input_texts])\n",
        "max_tr_len = mode([len(i) for i in target_texts])\n",
        "\n",
        "print(\"number of input words : \",num_in_words)\n",
        "print(\"number of target words : \",num_tr_words)\n",
        "print(\"maximum input length : \",max_in_len)\n",
        "print(\"maximum target length : \",max_tr_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YmQ3INA4Kq2",
        "outputId": "2741672f-fde5-4d38-db85-59da08e41378"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-dbf4d4b04c4c>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  texts = BeautifulSoup(texts, \"lxml\").text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of input words :  32198\n",
            "number of target words :  14171\n",
            "maximum input length :  74\n",
            "maximum target length :  17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the input and target text into 80:20 ratio or testing size of 20%.\n",
        "x_train,x_test,y_train,y_test=train_test_split(input_texts,target_texts,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "hIMgfKy_5Zzn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the tokenizer with all the words\n",
        "in_tokenizer = Tokenizer()\n",
        "in_tokenizer.fit_on_texts(x_train)\n",
        "tr_tokenizer = Tokenizer()\n",
        "tr_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "#convert text into sequence of integers\n",
        "#where the integer will be the index of that word\n",
        "x_train= in_tokenizer.texts_to_sequences(x_train) \n",
        "y_train= tr_tokenizer.texts_to_sequences(y_train) "
      ],
      "metadata": {
        "id": "gRum8TA15bao"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pad array of 0's if the length is less than the maximum length \n",
        "en_in_data= pad_sequences(x_train,  maxlen=max_in_len, padding='post') \n",
        "dec_data= pad_sequences(y_train,  maxlen=max_tr_len, padding='post')\n",
        "\n",
        "#decoder input data will not include the last word \n",
        "#i.e. 'eos' in decoder input data\n",
        "dec_in_data = dec_data[:,:-1]\n",
        "#decoder target data will be one time step ahead as it will not include\n",
        "# the first word i.e 'sos'\n",
        "dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]"
      ],
      "metadata": {
        "id": "GVaBITxq5v6H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session() \n",
        "latent_dim = 500\n",
        "\n",
        "#create input object of total number of input words\n",
        "en_inputs = Input(shape=(max_in_len,)) \n",
        "en_embedding = Embedding(num_in_words+1, latent_dim)(en_inputs) "
      ],
      "metadata": {
        "id": "j0mvEaJn524Y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create 3 stacked LSTM layer with the shape of hidden dimension\n",
        "#LSTM 1\n",
        "en_lstm1= LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "en_outputs1, state_h1, state_c1= en_lstm1(en_embedding) \n",
        "\n",
        "#LSTM2\n",
        "en_lstm2= LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "en_outputs2, state_h2, state_c2= en_lstm2(en_outputs1) \n",
        "\n",
        "#LSTM3\n",
        "en_lstm3= LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "en_outputs3 , state_h3 , state_c3= en_lstm3(en_outputs2)\n",
        "\n",
        "#encoder states\n",
        "en_states= [state_h3, state_c3]"
      ],
      "metadata": {
        "id": "goBOOdps5-u5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder. \n",
        "dec_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(num_tr_words+1, latent_dim) \n",
        "dec_embedding = dec_emb_layer(dec_inputs) \n",
        "\n",
        "#initialize decoder's LSTM layer with the output states of encoder\n",
        "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "dec_outputs, *_ = dec_lstm(dec_embedding,initial_state=en_states)"
      ],
      "metadata": {
        "id": "Lfu5owYg6Ixp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Attention layer\n",
        "attention =Attention()\n",
        "attn_out = attention([dec_outputs,en_outputs3])\n",
        "\n",
        "#Concatenate the attention output with the decoder ouputs\n",
        "merge=Concatenate(axis=-1, name='concat_layer1')([dec_outputs,attn_out])"
      ],
      "metadata": {
        "id": "a-zeMOtt6PSj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dense layer (output layer)\n",
        "dec_dense = Dense(num_tr_words+1, activation='softmax') \n",
        "dec_outputs = dec_dense(merge) "
      ],
      "metadata": {
        "id": "TqE4PUzD6VJJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mode class and model summary\n",
        "model = Model([en_inputs, dec_inputs], dec_outputs) \n",
        "model.summary()\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "model.compile( \n",
        "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] ) \n",
        "model.fit( \n",
        "    [en_in_data, dec_in_data],\n",
        "    dec_tr_data, \n",
        "    batch_size=512, \n",
        "    epochs=10, \n",
        "    validation_split=0.1,\n",
        "    )\n",
        "\n",
        "#Save model\n",
        "model.save(\"s2s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96p-EFhN6baw",
        "outputId": "15f783aa-307c-4e7c-d255-579fe7ed577f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 74)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 74, 500)      16099500    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 74, 500),    2002000     ['embedding[0][0]']              \n",
            "                                 (None, 500),                                                     \n",
            "                                 (None, 500)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 74, 500),    2002000     ['lstm[0][0]']                   \n",
            "                                 (None, 500),                                                     \n",
            "                                 (None, 500)]                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 500)    7086000     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 74, 500),    2002000     ['lstm_1[0][0]']                 \n",
            "                                 (None, 500),                                                     \n",
            "                                 (None, 500)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 500),  2002000     ['embedding_1[0][0]',            \n",
            "                                 (None, 500),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 500)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " attention (Attention)          (None, None, 500)    0           ['lstm_3[0][0]',                 \n",
            "                                                                  'lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            " concat_layer1 (Concatenate)    (None, None, 1000)   0           ['lstm_3[0][0]',                 \n",
            "                                                                  'attention[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 14172)  14186172    ['concat_layer1[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 45,379,672\n",
            "Trainable params: 45,379,672\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - 101s 705ms/step - loss: 1.7885 - accuracy: 0.7723 - val_loss: 1.4241 - val_accuracy: 0.7818\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - 94s 752ms/step - loss: 1.3322 - accuracy: 0.8277 - val_loss: 1.3128 - val_accuracy: 0.8333\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - 97s 778ms/step - loss: 1.3439 - accuracy: 0.8292 - val_loss: 1.3068 - val_accuracy: 0.8332\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - 98s 781ms/step - loss: 1.2991 - accuracy: 0.8314 - val_loss: 1.2950 - val_accuracy: 0.8306\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - 96s 770ms/step - loss: 1.2950 - accuracy: 0.8313 - val_loss: 1.2916 - val_accuracy: 0.8333\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - 98s 781ms/step - loss: 1.2908 - accuracy: 0.8314 - val_loss: 1.2915 - val_accuracy: 0.8333\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - 96s 770ms/step - loss: 1.2865 - accuracy: 0.8316 - val_loss: 1.2921 - val_accuracy: 0.8299\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - 98s 780ms/step - loss: 1.2831 - accuracy: 0.8317 - val_loss: 1.2884 - val_accuracy: 0.8338\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - 97s 780ms/step - loss: 1.2795 - accuracy: 0.8316 - val_loss: 1.2876 - val_accuracy: 0.8335\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - 98s 781ms/step - loss: 1.2834 - accuracy: 0.8316 - val_loss: 1.2839 - val_accuracy: 0.8297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder inference\n",
        "latent_dim=500\n",
        "#load the model\n",
        "model = models.load_model(\"s2s\")\n",
        "\n",
        "#construct encoder model from the output of 6 layer i.e.last LSTM layer\n",
        "en_outputs,state_h_enc,state_c_enc = model.layers[6].output\n",
        "en_states=[state_h_enc,state_c_enc]\n",
        "#add input and state from the layer.\n",
        "en_model = Model(model.input[0],[en_outputs]+en_states)"
      ],
      "metadata": {
        "id": "tiu-YlTKrv4N"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder inference\n",
        "#create Input object for hidden and cell state for decoder\n",
        "#shape of layer with hidden or latent dimension\n",
        "dec_state_input_h = Input(shape=(latent_dim,))\n",
        "dec_state_input_c = Input(shape=(latent_dim,))\n",
        "dec_hidden_state_input = Input(shape=(max_in_len,latent_dim))\n",
        "\n",
        "# Get the embeddings and input layer from the model\n",
        "dec_inputs = model.input[1]\n",
        "dec_emb_layer = model.layers[5]\n",
        "dec_lstm = model.layers[7]\n",
        "dec_embedding= dec_emb_layer(dec_inputs)\n",
        "\n",
        "#add input and initialize LSTM layer with encoder LSTM states.\n",
        "dec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, initial_state=[dec_state_input_h,dec_state_input_c])"
      ],
      "metadata": {
        "id": "GQijihiGsZpG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Attention layer\n",
        "attention = model.layers[8]\n",
        "attn_out2 = attention([dec_outputs2,dec_hidden_state_input])\n",
        "\n",
        "merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out2])"
      ],
      "metadata": {
        "id": "vrAC9TDmtGYt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dense layer\n",
        "dec_dense = model.layers[10]\n",
        "dec_outputs2 = dec_dense(merge2)\n",
        "\n",
        "# Finally define the Model Class\n",
        "dec_model = Model(\n",
        "[dec_inputs] + [dec_hidden_state_input,dec_state_input_h,dec_state_input_c],\n",
        "[dec_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "q7z6MlsWtMZy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dictionary with a key as index and value as words.\n",
        "reverse_target_word_index = tr_tokenizer.index_word\n",
        "reverse_source_word_index = in_tokenizer.index_word\n",
        "target_word_index = tr_tokenizer.word_index\n",
        "reverse_target_word_index[0]=' '\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    #get the encoder output and states by passing the input sequence\n",
        "    en_out, en_h, en_c= en_model.predict(input_seq)\n",
        "\n",
        "    #target sequence with inital word as 'sos'\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_word_index['sos']\n",
        "\n",
        "    #if the iteration reaches the end of text than it will be stop the iteration\n",
        "    stop_condition = False\n",
        "    #append every predicted word in decoded sentence\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition: \n",
        "        #get predicted output, hidden and cell state.\n",
        "        output_words, dec_h, dec_c= dec_model.predict([target_seq] + [en_out,en_h, en_c])\n",
        "        \n",
        "        #get the index and from the dictionary get the word for that index.\n",
        "        word_index = np.argmax(output_words[0, -1, :])\n",
        "        text_word = reverse_target_word_index[word_index]\n",
        "        decoded_sentence += text_word +\" \"\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find a stop word or last word.\n",
        "        if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n",
        "          stop_condition = True\n",
        "        \n",
        "        #update target sequence to the current word index.\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = word_index\n",
        "        en_h, en_c = dec_h, dec_c\n",
        "    \n",
        "    #return the deocded sentence\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "1kfvsZietSfZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_review = input(\"Enter : \")\n",
        "print(\"Review :\",inp_review)\n",
        "\n",
        "inp_review = clean(inp_review,\"inputs\")\n",
        "inp_review = ' '.join(inp_review)\n",
        "\n",
        "inp_x= in_tokenizer.texts_to_sequences([inp_review]) \n",
        "inp_x= pad_sequences(inp_x,  maxlen=max_in_len, padding='post')\n",
        "\n",
        "summary=decode_sequence(inp_x.reshape(1,max_in_len))\n",
        "\n",
        "if 'eos' in summary :\n",
        "  summary=summary.replace('eos','')\n",
        "print(\"\\nPredicted summary:\",summary);print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K01k-jVxtu4t",
        "outputId": "25dccfc7-1d76-4f3e-c8cb-844f26ce6356"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter : good choclate tastes good affordable\n",
            "Review : good choclate tastes good affordable\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "\n",
            "Predicted summary: best  \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "WuFYCMnFtiyq"
      }
    }
  ]
}